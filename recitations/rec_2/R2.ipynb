{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Introduction to PyTorch\n","\n","## What is PyTorch?\n","\n","PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR) that provides a flexible and intuitive framework for deep learning. It has gained significant popularity in both academic research and industry applications due to its dynamic computational graph, intuitive API, and seamless integration with the Python ecosystem.\n","\n","### Key Characteristics of PyTorch:\n","\n","1. **Pythonic Nature**: PyTorch feels natural to Python programmers, making it easier to learn and integrate with existing Python codebases and libraries.\n","\n","2. **Dynamic Computational Graph**: Unlike static graph frameworks, PyTorch builds graphs on-the-fly (define-by-run), allowing for more flexible model architecture and easier debugging.\n","\n","3. **Tensor Computation**: At its core, PyTorch provides a tensor data structure similar to NumPy arrays but with the ability to run on GPUs for accelerated computing.\n","\n","4. **Automatic Differentiation**: PyTorch includes an automatic differentiation engine called \"autograd\" that enables automatic computation of gradients needed for training neural networks.\n","\n","5. **Production Ready**: With TorchScript and other deployment tools, PyTorch models can be easily transitioned from research to production environments.\n","\n","## PyTorch vs. NumPy\n","\n","If you're already familiar with NumPy from our first recitation, you'll find many similarities with PyTorch:\n","\n","| Feature | NumPy | PyTorch |\n","|---------|-------|---------|\n","| Primary data structure | ndarray | Tensor |\n","| Device support | CPU only | CPU and GPU |\n","| Automatic differentiation | No | Yes |\n","| Broadcasting behavior | Yes | Yes |\n","| API style | Similar | Similar |\n","\n","The main difference is that PyTorch tensors can leverage GPU acceleration and automatically track computations for gradient calculation, making them ideal for deep learning.\n","\n","## What We'll Cover in This Recitation\n","\n","1. **PyTorch Tensors**: Creating, manipulating, and understanding the fundamental data structure\n","2. **Automatic Differentiation**: Understanding how PyTorch computes gradients automatically\n","3. **Building Neural Networks**: Using the nn module to create neural network architectures\n","4. **Training Models**: The training loop, optimizers, and loss functions\n","5. **Data Loading**: Using DataLoader and Dataset classes for efficient data handling\n","6. **Practical Examples**: Hands-on implementations of common deep learning tasks\n","\n","By the end of this recitation, you'll have a solid understanding of PyTorch's core components and be able to implement basic deep learning models.\n","\n","Let's get started!"],"metadata":{"id":"KM-_TV70JuLt"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"DCzd68TfLBRS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Linear regresion in numpy vs pytorch"],"metadata":{"id":"uzjekNRMKeop"}},{"cell_type":"markdown","source":["### Numpy"],"metadata":{"id":"RrlCXmc2K9o5"}},{"cell_type":"code","source":["class NumpyDataLoader:\n","    def __init__(self, X, y, batch_size=32, shuffle=True):\n","        \"\"\"\n","        Initialize a simple NumPy-based data loader.\n","\n","        Parameters:\n","            X (ndarray): Input data, shape (N, ...)\n","            y (ndarray): Labels, shape (N,)\n","            batch_size (int): Number of samples per batch\n","            shuffle (bool): Whether to shuffle data each epoch\n","        \"\"\"\n","        self.X = X\n","        self.y = y\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.current = 0\n","        self.num_samples = X.shape[0]\n","        self.indices = np.arange(self.num_samples)\n","\n","    def __iter__(self):\n","        if self.shuffle:\n","            np.random.shuffle(self.indices)\n","        self.current = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.current >= self.num_samples:\n","            raise StopIteration\n","        start = self.current\n","        end = min(start + self.batch_size, self.num_samples)\n","        batch_idx = self.indices[start:end]\n","        self.current = end\n","        return self.X[batch_idx], self.y[batch_idx]\n","\n","    def __len__(self):\n","        return int(np.ceil(self.num_samples / self.batch_size))"],"metadata":{"id":"0xhZsuaNJg8z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate 3D data (same as before)\n","num_features = 1\n","num_points = 250\n","x = np.random.uniform(-1, 1, size=(num_points, num_features))\n","m = np.random.random(num_features)\n","b_true = np.random.random(1)\n","noise = 0.050\n","y = x @ m + b_true + np.random.normal(0, noise, size=num_points)\n","print(f\"true m: {m}, true b: {b_true}\")\n"],"metadata":{"id":"PW_bzaT88S2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(x, y, 'o')\n","plt.plot(x, x @ m + b_true, color='r')\n","plt.xlabel('x')\n","plt.ylabel('y')"],"metadata":{"id":"hoL4lDynSm1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_loader = NumpyDataLoader(x, y, batch_size=8, shuffle=True)\n","len(data_loader)"],"metadata":{"id":"RuT_mffl8S2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize weights\n","w = np.random.random(num_features) # one for the slope (m) and second for bais (b)\n","b = np.random.random()\n","\n","# Hyperparameters\n","lr = 0.001\n","epochs = 100"],"metadata":{"id":"qrWnBdET8S2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epochs):\n","    for x_batch, y_batch in data_loader:\n","        y_pred = x_batch @ w + b\n","\n","        # Errors\n","        error = y_pred - y_batch\n","\n","        # Gradients (vectorized!)\n","        grad_w = 2 * np.mean(x_batch.T * error, axis=1)\n","        grad_b = 2 * np.mean(error)\n","\n","        # Update parameters\n","        w -= lr * grad_w\n","        b -= lr * grad_b\n","\n","    if epoch % (epochs//10) == 0 or epoch == epochs - 1:\n","      loss = np.mean((x @ w + b - y) ** 2)\n","      print(f\"Epoch {epoch+1}: Loss = {loss:.5f}\")\n","      # print(w, b)\n","\n","print()\n","print(f\"bais -> True : {b_true[0]:.3f}, Pred: {b:.3f}\")\n","for i, pair in enumerate(zip(m,w)):\n","  print(f\"w{i} -> True: {pair[0]:.3f}, Pred: {pair[1]:.3f}\")\n","\n"],"metadata":{"id":"ymHiQvTD8S2k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pytorch"],"metadata":{"id":"vf4kaLXSLP9c"}},{"cell_type":"code","source":["# Custom Dataset class for PyTorch\n","class SimpleDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.x = torch.FloatTensor(x)\n","        self.y = torch.FloatTensor(y)\n","\n","    def __getitem__(self, idx):\n","        return self.x[idx], self.y[idx]\n","\n","    def __len__(self):\n","        return len(self.x)\n"],"metadata":{"id":"whs27BaELXdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define dimensions\n","num_features = 1\n","num_points = 250\n","\n","# Generate input features using uniform distribution between -1 and 1\n","x = torch.rand(num_points, num_features) * 2 - 1  # scales [0,1] to [-1,1]\n","# Generate true weights using random uniform [0,1]\n","m_true = torch.rand(num_features)\n","\n","# Define true bias\n","b_true = torch.rand(1)\n","noise = .05\n","\n","# Generate target values using PyTorch operations\n","y = x @ m_true + b_true + torch.normal(0, noise, size=(num_points,))\n","# Alternative using @ operator: y = x @ m_true + b_true + torch.normal(0, noise, size=(num_points,))\n","\n","print(f\"true m: {m}, true b: {b_true}\")\n"],"metadata":{"id":"Qjs6lUsQLrVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(x, y, 'o')\n","plt.plot(x, x @ m + b_true, color='r')\n","plt.xlabel('x')\n","plt.ylabel('y')\n"],"metadata":{"id":"FYP406Mji0JD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 8\n","shuffle = True\n","dataset = SimpleDataset(x, y)\n","data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n","len(data_loader)\n","lr = 0.0025\n","epochs = 100\n","w = torch.randn(num_features, requires_grad=True)\n","b = torch.randn(1, requires_grad=True)\n","print(w)\n","print(w)"],"metadata":{"id":"ML3NORzYMAxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Option 1: Manual optimization"],"metadata":{"id":"HCrO4NOeOG9y"}},{"cell_type":"code","source":["# Option 1: Manual optimization\n","for epoch in range(epochs):\n","    for x_batch, y_batch in data_loader:\n","\n","        # Forward pass\n","        y_pred = torch.matmul(x_batch, w) + b\n","\n","        # Compute loss\n","        loss = torch.mean((y_pred - y_batch) ** 2)\n","\n","        # Backward pass (compute gradients)\n","        loss.backward()\n","\n","        # Update parameters manually\n","        with torch.no_grad():  # No gradient tracking during updates\n","            w -= lr * w.grad\n","            b -= lr * b.grad\n","\n","            # Reset gradients to zero (important!)\n","            w.grad.zero_()\n","            b.grad.zero_()\n","\n","    if epoch % (epochs//10) == 0 or epoch == epochs - 1:\n","        # Compute loss on entire dataset\n","        with torch.no_grad():\n","            y_pred_all = torch.matmul(torch.FloatTensor(x), w) + b\n","            loss_all = torch.mean((y_pred_all - torch.FloatTensor(y)) ** 2)\n","            print(f\"Epoch {epoch+1}: Loss = {loss_all:.5f}\")\n","\n","print()\n","print(f\"bias -> True: {b_true[0]:.3f}, Pred: {b.item():.3f}\")\n","for i, (true_val, pred_val) in enumerate(zip(m_true, w.detach().numpy())):\n","    print(f\"w{i} -> True: {true_val:.3f}, Pred: {pred_val:.3f}\")"],"metadata":{"id":"TEWs-JEoMlKt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Option 2: Using PyTorch's built-in optimizers (much simpler!)"],"metadata":{"id":"BxMdh-XyOKsS"}},{"cell_type":"code","source":["print(\"\\nNow with PyTorch optimizer:\\n\")\n","\n","# Reset weights\n","w = torch.randn(num_features, requires_grad=True)\n","b = torch.randn(1, requires_grad=True)\n","\n","# Create optimizer\n","optimizer = torch.optim.SGD([w, b], lr=lr)\n","\n","for epoch in range(epochs):\n","    for x_batch, y_batch in data_loader:\n","        # Forward pass\n","        y_pred = torch.matmul(x_batch, w) + b\n","\n","        # Compute loss\n","        loss = torch.mean((y_pred - y_batch) ** 2)\n","\n","        # Zero gradients, backward pass, update\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if epoch % (epochs//10) == 0 or epoch == epochs - 1:\n","        # Compute loss on entire dataset\n","        with torch.no_grad():\n","            y_pred_all = torch.matmul(torch.FloatTensor(x), w) + b\n","            loss_all = torch.mean((y_pred_all - torch.FloatTensor(y)) ** 2)\n","            print(f\"Epoch {epoch+1}: Loss = {loss_all:.5f}\")\n","\n","print()\n","print(f\"bias -> True: {b_true[0]:.3f}, Pred: {b.item():.3f}\")\n","for i, (true_val, pred_val) in enumerate(zip(m_true, w.detach().numpy())):\n","    print(f\"w{i} -> True: {true_val:.3f}, Pred: {pred_val:.3f}\")\n"],"metadata":{"id":"31II3HU1NcKh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Option 3: Using nn.Linear for even more PyTorch-like implementation\n"],"metadata":{"id":"3q6h-N0pOOQZ"}},{"cell_type":"code","source":["print(\"\\nNow with PyTorch nn.Linear module:\\n\")\n","\n","# Define model using PyTorch's nn Module\n","class LinearRegressionModel(torch.nn.Module):\n","    def __init__(self, input_dim):\n","        super(LinearRegressionModel, self).__init__()\n","        self.linear = torch.nn.Linear(input_dim, 1)  # One output: the prediction\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# Create model, loss, and optimizer\n","model = LinearRegressionModel(num_features)\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","\n","# Training loop\n","for epoch in range(epochs):\n","    for x_batch, y_batch in data_loader:\n","        # Forward pass\n","        y_pred = model(x_batch).squeeze()\n","\n","        # Compute loss\n","        loss = criterion(y_pred, y_batch)\n","\n","        # Zero gradients, backward pass, update\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if epoch % (epochs//10) == 0 or epoch == epochs - 1:\n","        # Compute loss on entire dataset\n","        with torch.no_grad():\n","            y_pred_all = model(torch.FloatTensor(x)).squeeze()\n","            loss_all = criterion(y_pred_all, torch.FloatTensor(y))\n","            print(f\"Epoch {epoch+1}: Loss = {loss_all:.5f}\")\n","\n","# Extract weights and bias for comparison\n","with torch.no_grad():\n","    learned_weights = model.linear.weight.data.squeeze().numpy()\n","    learned_bias = model.linear.bias.item()\n","\n","print()\n","print(f\"bias -> True: {b_true[0]:.3f}, Pred: {learned_bias:.3f}\")\n","for i, (true_val, pred_val) in enumerate(zip(m_true, [learned_weights])):\n","    print(f\"w{i} -> True: {true_val:.3f}, Pred: {pred_val:.3f}\")"],"metadata":{"id":"xYJR8SG4LRxI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## What did we used here...."],"metadata":{"id":"R56_Qii7baLQ"}},{"cell_type":"markdown","source":["### PyTorch Tensors"],"metadata":{"id":"djDmIca8fvNI"}},{"cell_type":"markdown","source":["\n","\n","#### What is a Tensor?\n","\n","A tensor is the fundamental data structure in PyTorch, similar to NumPy's ndarray but with additional capabilities. Tensors are multi-dimensional arrays that can be processed on CPUs or GPUs and are designed to support automatic differentiation.\n","\n"],"metadata":{"id":"V3R631docA7a"}},{"cell_type":"markdown","source":["#### Initialize tensors"],"metadata":{"id":"C7O_wZ7wdXUp"}},{"cell_type":"code","source":["# From lists\n","tensor_from_list = torch.tensor([1, 2, 3, 4])\n","\n","# From nested lists (2D tensor)\n","tensor_2d = torch.tensor([[1, 2], [3, 4]])\n","\n","# Zeros tensor\n","zeros = torch.zeros(3, 4)  # 3x4 tensor of zeros\n","\n","# Ones tensor\n","ones = torch.ones(2, 3)    # 2x3 tensor of ones\n","\n","# Random tensor (uniform distribution)\n","random_uniform = torch.rand(2, 2)   # Values between 0 and 1\n","\n","# Random tensor (normal distribution)\n","random_normal = torch.randn(2, 2)   # Mean 0, variance 1\n","\n","# Create tensor with specific values\n","like_this = torch.ones_like(tensor_2d)  # Same size as tensor_2d, filled with ones\n","\n","# Creating a range\n","range_tensor = torch.arange(0, 10, step=1)  # 0 to 9\n","\n","numpy_array = np.array([1, 2, 3])\n","tensor_from_numpy = torch.from_numpy(numpy_array)  # Shares memory with numpy_array"],"metadata":{"id":"34tL_Q8ec8yq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Tensor properties"],"metadata":{"id":"jfBnVz_8dcF4"}},{"cell_type":"code","source":["x = torch.randn(3, 4)\n","\n","# Shape\n","print(x.shape)      # torch.Size([3, 4])\n","print(x.size())     # torch.Size([3, 4])\n","\n","# Data type\n","print(x.dtype)      # torch.float32\n","\n","# Number of dimensions\n","print(x.dim())      # 2\n","\n","# Number of elements\n","print(x.numel())    # 12\n","\n","# Device where tensor is stored\n","print(x.device)     # cpu (or cuda:0 if on GPU)\n","x_gpu = x.to(device)\n","print(x_gpu.device)     # cpu (or cuda:0 if on GPU)\n","# Get the underlying data as a Python number, list, or NumPy array\n","if x.numel() == 1:\n","    print(x.item())  # For single-element tensors\n","print(x.tolist())    # Convert to nested Python lists\n","print(x.numpy())     # Convert to NumPy array (CPU only)"],"metadata":{"id":"n7UhpJFGdHyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Reshaping and Transposing\n"],"metadata":{"id":"vGb2Bgr9d6tR"}},{"cell_type":"code","source":["# Reshaping and Transposing\n","x = torch.randn(3, 4)\n","\n","# Reshape\n","y = x.reshape(12)          # Flattens to 1D tensor with 12 elements\n","y = x.reshape(2, 6)        # Reshapes to 2x6 tensor\n","y = x.view(12)             # view is similar to reshape but shares memory (don't count on it)\n","\n","# Transpose\n","z = x.t()                  # Transpose (swaps dimensions)\n","z = x.permute(1, 0)        # More general version of transpose"],"metadata":{"id":"tLPaU3OhdMXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Autograd"],"metadata":{"id":"ghz67Z-yf7v7"}},{"cell_type":"markdown","source":["\n","#### What is a Computation Graph?\n","\n","A computation graph is a directed graph that represents a sequence of mathematical operations. In PyTorch, computation graphs are built dynamically as operations are performed on tensors, enabling automatic differentiation.\n","\n","[MiniGrad by Andrej Karpathy](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLXYLzZ3XzIbi4lL43O6fIU_ojuZwBO6vi)"],"metadata":{"id":"_YIxivBFgRO2"}},{"cell_type":"markdown","source":["#### Dynamic vs. Static Graphs\n","\n","PyTorch uses a **dynamic computation graph** approach:\n","\n","- **Dynamic Graph (PyTorch)**: Built on-the-fly during forward pass; can be different each time code is run\n","- **Static Graph (TensorFlow 1.x)**: Built once, then executed repeatedly\n","\n","The dynamic approach allows for:\n","- More intuitive debugging\n","- Use of standard Python control flow (if/else, loops)\n","- Easier handling of variable-length inputs\n","- More natural integration with Python code\n"],"metadata":{"id":"lRyKFoikgbQx"}},{"cell_type":"markdown","source":["#### Autograd: Automatic Differentiation\n","\n","PyTorch's autograd package provides automatic differentiation for operations on tensors. It records operations as they happen, building a computation graph, and then automatically computes gradients."],"metadata":{"id":"wc21ZEIxgfnk"}},{"cell_type":"markdown","source":["\n","#### How Autograd Works\n","\n","1. Create tensors with `requires_grad=True`\n","2. Perform operations on these tensors\n","3. Call `.backward()` on the output\n","4. Access gradients via the `.grad` attribute on input tensors"],"metadata":{"id":"Vqp7OSccgwaY"}},{"cell_type":"code","source":["x = torch.tensor([2.0], requires_grad=True)\n","x"],"metadata":{"id":"ROJo185kg41q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example: Simple autograd usage\n","y = 3 * x**2\n","y.backward()\n","print(x.grad)  # Output: tensor([12.])  (derivative of 3xÂ² is 6x, and x=2, so 6*2=12)"],"metadata":{"id":"2swqPmJZgyI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([1.0], requires_grad=True)\n","s = nn.Sigmoid()\n","a = s(x)\n","a.backward()\n","print(f\"a: {a}\")\n","print(f\"derivative: {a.item() * (1 - a.item())}\")\n","print(f\"x.grad: {x.grad}\")"],"metadata":{"id":"qngT9I0MiTgB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataLoader"],"metadata":{"id":"oNLZFM6EkRB7"}},{"cell_type":"markdown","source":["#### The Data Loading Pipeline\n","\n","Efficiently loading data is crucial for training deep learning models. PyTorch provides a powerful and flexible data loading mechanism through two main components:\n","\n","1. **Dataset**: Represents a collection of data samples\n","2. **DataLoader**: Wraps a Dataset and provides batching, shuffling, and parallel data loading\n","\n","This pipeline allows you to:\n","- Handle large datasets that don't fit in memory\n","- Process data on-the-fly during training\n","- Load data efficiently with multiple CPU workers\n","- Easily implement custom data augmentation"],"metadata":{"id":"hUANB97Sk0AZ"}},{"cell_type":"markdown","source":["#### Built-in Datasets\n","\n","PyTorch provides many pre-implemented datasets in `torchvision`, `torchaudio`, and `torchtext`:\n"],"metadata":{"id":"6NMc8XE4lDUF"}},{"cell_type":"code","source":["from torchvision import datasets\n","\n","# Load MNIST dataset\n","mnist_dataset = datasets.MNIST(\n","    root='./data',\n","    train=True,\n","    download=True,\n",")"],"metadata":{"id":"x-dpbU7clFSN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### Dataset Types\n","\n","PyTorch supports different types of datasets:\n","\n","##### **Map-style Datasets**\n","- Implements `__getitem__()` and `__len__()`\n","- Supports random access using indices\n","- Example: Image datasets, tabular data\n","\n","##### **Iterable-style Datasets**\n","- Inherits from `IterableDataset`\n","- Implements `__iter__()` instead of `__getitem__()`\n","- Used for data streams that can't be randomly accessed\n","- Example: Data read from database, streaming data\n"],"metadata":{"id":"nr5Jq1j6lZsK"}},{"cell_type":"markdown","source":["#### The DataLoader Class\n","\n","The `DataLoader` combines a dataset with features for efficient training:\n","\n","**Key Parameters**\n","\n","##### `batch_size`\n","- Number of samples in each batch\n","- Higher values use more memory but can increase training speed\n","- Typically powers of 2 (16, 32, 64, 128, etc.)\n","\n","##### `shuffle`\n","- When `True`, shuffles data at the start of each epoch\n","- Important for stochastic gradient descent\n","- Set to `False` for evaluation or when order matters\n","\n","##### `num_workers`\n","- Number of subprocesses for data loading\n","- `0` means data loading in the main process\n","- Increase to speed up data loading (typically 4-8 for single machine)\n","\n","##### `drop_last`\n","- When `True`, drops the last incomplete batch\n","- Useful when batch normalization is used to ensure all batches have the same size\n","\n","##### `pin_memory`\n","- When `True`, tensors are copied to pinned (page-locked) memory\n","- Makes CPU to GPU transfers faster\n","- Recommended when using GPU\n","\n","##### `collate_fn`\n","- Function to merge a list of samples into a mini-batch\n","- Customize for special handling (e.g., variable-length sequences)\n","```python\n","def my_collate_fn(batch):\n","    # Custom processing for each batch\n","    data = [item[0] for item in batch]\n","    targets = [item[1] for item in batch]\n","    # Do custom processing\n","    return torch.stack(data), torch.stack(targets)\n","\n","  dataloader = DataLoader(dataset, batch_size=32, collate_fn=my_collate_fn)\n","```"],"metadata":{"id":"sz6xJeiqlyNE"}},{"cell_type":"code","source":["dataloader = DataLoader(\n","    dataset,               # Your dataset\n","    batch_size=32,         # Number of samples per batch\n","    shuffle=True,          # Shuffle data at each epoch\n","    num_workers=4,         # Number of subprocesses for data loading\n","    drop_last=False,       # Drop the last incomplete batch\n","    pin_memory=True        # Pin memory for faster GPU transfer\n",")\n","for x_batch, y_batch in dataloader:\n","    print(x_batch.shape, y_batch.shape)\n","    break"],"metadata":{"id":"QfQ3fh9dlsx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### Basic Training Loop:\n","\n"],"metadata":{"id":"bERc8L9dmtE2"}},{"cell_type":"code","source":["# Iterate through batches\n","num_epochs = 100\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","for epoch in range(num_epochs):\n","    for batch_idx, (data, targets) in enumerate(dataloader):\n","        # Move to GPU if available\n","        data, targets = data.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(data)\n","\n","        # Compute loss\n","        loss = criterion(outputs, targets)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 100 == 0:\n","            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')"],"metadata":{"id":"rIUJmMWtmwVj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Troch.nn"],"metadata":{"id":"538B_4LRm_CY"}},{"cell_type":"markdown","source":["#### Introduction to torch.nn\n","\n","The `torch.nn` module is the cornerstone of building neural networks in PyTorch. It provides the building blocks for creating everything from simple linear models to complex deep neural networks. Unlike direct tensor operations, `torch.nn` offers higher-level abstractions that handle parameter management, forward propagation, and integration with optimizers."],"metadata":{"id":"DR5zHlYzoSK0"}},{"cell_type":"markdown","source":["#### Core Components of torch.nn"],"metadata":{"id":"3QVaU3B0ogTp"}},{"cell_type":"markdown","source":["##### 1. nn.Module: The Foundation of All Models\n","\n","`nn.Module` is the base class for all neural network modules in PyTorch. Every model you create should inherit from this class.\n"],"metadata":{"id":"7y4epbldojUK"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        # Define layers here\n","\n","    def forward(self, x):\n","        # Define forward pass\n","        return output"],"metadata":{"id":"_CPont6OomNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Key features of `nn.Module`:\n","\n","- **Automatic Parameter Tracking**: All parameters (defined using `nn.Parameter` or added as submodules) are automatically tracked\n","- **Nested Structure**: Can contain other modules for hierarchical organization\n","- **Mode Switching**: Provides `train()` and `eval()` methods to switch between training and evaluation modes\n","- **Device Movement**: Use `.to(device)` to move all parameters to CPU/GPU\n","\n","Example of a simple model:"],"metadata":{"id":"x-WBXYabovFE"}},{"cell_type":"code","source":["class SimpleNetwork(nn.Module):\n","    def __init__(self, input_size=10, hidden_size=10, output_size=10):\n","        super(SimpleNetwork, self).__init__()\n","        self.layer1 = nn.Linear(input_size, hidden_size)\n","        self.activation = nn.ReLU()\n","        self.layer2 = nn.Linear(hidden_size, output_size)\n","\n","          # Register them with names\n","        self.register_module('layer1', self.layer1)\n","        self.register_module('layer2', self.layer2)\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.activation(x)\n","        x = self.layer2(x)\n","        return x\n","\n","model = SimpleNetwork()\n","for name, param in model.named_parameters():\n","    print(name, param.shape)"],"metadata":{"id":"vDHlh33Oowo0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","##### 2. nn.Parameter: Trainable Parameters\n","\n","`nn.Parameter` is a special kind of Tensor that is automatically registered as a parameter when assigned as an attribute of an `nn.Module`."],"metadata":{"id":"4cG96Iwfo21K"}},{"cell_type":"code","source":["class CustomLayer(nn.Module):\n","    def __init__(self, input_size=10):\n","        super(CustomLayer, self).__init__()\n","        # This will be automatically registered as a parameter\n","        self.weights = nn.Parameter(torch.randn(input_size, input_size))\n","        # This regular tensor will NOT be registered as a parameter\n","        self.non_param = torch.randn(input_size, input_size)\n","\n","    def forward(self, x):\n","        return x @ self.weights\n","\n","model = CustomLayer()\n","for name, param in model.named_parameters():\n","    print(name, param.shape)"],"metadata":{"id":"hJy4A4_lo4Ar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","##### 3. Common Layer Types"],"metadata":{"id":"OhTP7fPAo_Eg"}},{"cell_type":"markdown","source":["###### Linear Layers"],"metadata":{"id":"lxL4WeTxpEzf"}},{"cell_type":"code","source":["# Basic linear (fully connected) layer\n","linear = nn.Linear(in_features=10, out_features=5, bias=True)\n","\n","# Input: [batch_size, 10]\n","# Output: [batch_size, 5]"],"metadata":{"id":"y6ISIYXFo-ih"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Convolutional Layers\n","\n","\n"],"metadata":{"id":"i9j58_rDpJM1"}},{"cell_type":"code","source":["# 2D Convolution\n","conv2d = nn.Conv2d(\n","    in_channels=3,      # Input channels (e.g., RGB)\n","    out_channels=16,    # Output feature maps\n","    kernel_size=3,      # Filter size\n","    stride=1,           # Step size\n","    padding=1,          # Zero-padding\n","    bias=True           # Whether to include bias\n",")\n","# Input: [batch_size, 3, height, width]\n","# Output: [batch_size, 16, height, width] (with padding=1)"],"metadata":{"id":"7uxHwBZupQsP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Pooling Layers"],"metadata":{"id":"-5ZcIz5LpbJe"}},{"cell_type":"code","source":["# Max pooling\n","maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","# Average pooling\n","avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","# Both reduce spatial dimensions by half"],"metadata":{"id":"Jkreb_izpgIy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Normalization Layers\n"],"metadata":{"id":"wOuThwICpiPB"}},{"cell_type":"code","source":["# Batch Normalization\n","batchnorm = nn.BatchNorm2d(num_features=16)  # For 2D data (images)\n","\n","# Layer Normalization\n","layernorm = nn.LayerNorm(normalized_shape=[16, 32, 32])  # Normalize over last 3 dims"],"metadata":{"id":"s1F5ivDbpktt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Dropout Layers\n"],"metadata":{"id":"4gI5itkmpzmY"}},{"cell_type":"code","source":["# Dropout - randomly zero out elements (regularization technique)\n","dropout = nn.Dropout(p=0.5)  # 50% probability of zeroing\n","dropout = nn.Dropout2d(p=0.5)  # 50% probability of zeroing"],"metadata":{"id":"p2rqJkgXp2OU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Activation Functions"],"metadata":{"id":"iPowXWrxp692"}},{"cell_type":"code","source":["# Common activation functions\n","relu = nn.ReLU()\n","leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n","sigmoid = nn.Sigmoid()\n","tanh = nn.Tanh()\n","softmax = nn.Softmax(dim=1)  # Apply softmax along dim 1"],"metadata":{"id":"-RjfPznQp-Wx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","##### 4. Containers\n"],"metadata":{"id":"UxUQOPEcqEsk"}},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.layer1 = nn.Linear(10, 20)\n","        self.relu1 = nn.ReLU()\n","        self.layer2 = nn.Linear(20, 15)\n","        self.relu2 = nn.ReLU()\n","        self.layer3 = nn.Linear(15, 5)\n","\n","    def forward(self, x):\n","        x = self.relu1(self.layer1(x))\n","        x = self.relu2(self.layer2(x))\n","        x = self.layer3(x)\n","        return x\n","\n","\n","class DynamicModel(nn.Module):\n","    def __init__(self, ):\n","        super(DynamicModel, self).__init__()\n","        self.layers = nn.ModuleDict({\n","            'layer1': nn.Linear(10, 20),\n","            'relu1': nn.ReLU(),\n","            'layer2': nn.Linear(20, 15),\n","            'relu2': nn.ReLU(),\n","            'layer3': nn.Linear(15, 5)\n","        })\n","\n","    def forward(self, x):\n","        for layer_name in self.layers:\n","            x = self.layers[layer_name](x)\n","        return x\n","\n","\n","# Equivalent to:\n","model1 = nn.Sequential(\n","    nn.Linear(10, 20),\n","    nn.ReLU(),\n","    nn.Linear(20, 15),\n","    nn.ReLU(),\n","    nn.Linear(15, 5)\n",")\n","\n","model2  = Model()\n","model3 = DynamicModel()\n","\n","x = torch.randn(1, 10)\n","print(model1(x).shape)\n","print(model2(x).shape)\n","print(model3(x).shape)"],"metadata":{"id":"kW4YNOS7qJ1N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","##### 5. Loss Functions (nn.functional)\n","\n","PyTorch provides various loss functions in `torch.nn.functional` (often imported as F):\n","\n","```python\n","import torch.nn.functional as F\n","\n","# Cross Entropy Loss (for classification)\n","loss = F.cross_entropy(predictions, targets)\n","\n","# Mean Squared Error (for regression)\n","loss = F.mse_loss(predictions, targets)\n","\n","# Binary Cross Entropy (for binary classification)\n","loss = F.binary_cross_entropy(predictions, targets)\n","\n","# L1 Loss (Mean Absolute Error)\n","loss = F.l1_loss(predictions, targets)\n","```\n","\n","These are also available as modules:\n","\n","```python\n","# As modules (stateful)\n","criterion = nn.CrossEntropyLoss()\n","loss = criterion(predictions, targets)\n","```"],"metadata":{"id":"zXdi2ng9qOcS"}},{"cell_type":"markdown","source":["\n","\n","#### Saving and Loading Models\n","\n","```python\n","# Save the entire model\n","torch.save(model, 'model.pth')\n","\n","# Save just the state dict (recommended)\n","torch.save(model.state_dict(), 'model_state_dict.pth')\n","\n","# Load entire model\n","loaded_model = torch.load('model.pth')\n","\n","# Load state dict into an instance\n","model = SimpleNetwork(input_size, hidden_size, output_size)\n","model.load_state_dict(torch.load('model_state_dict.pth'))\n","model.eval()  # Set to evaluation mode\n","```"],"metadata":{"id":"nqUOtHMxoNZX"}},{"cell_type":"markdown","source":["###  Building a Simple Classifier in PyTorch"],"metadata":{"id":"NDHBmEyNs0Tq"}},{"cell_type":"markdown","source":["## Exercise: Logistic Regression Classifier\n","\n","In this exercise, we'll build a complete logistic regression classifier in PyTorch, combining all the concepts we've learned so far."],"metadata":{"id":"Si1flI-ouo36"}},{"cell_type":"markdown","source":["\n","### 1. Generating Two-Class Blob Data"],"metadata":{"id":"N36EfyE3u9rD"}},{"cell_type":"code","source":["from sklearn.datasets import make_blobs"],"metadata":{"id":"VWtk5dbKuoWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","n_features = 2\n","# Generate synthetic data: two blobs for binary classification\n","X_np, y_np = make_blobs(n_samples=1000, centers=2, n_features=n_features, random_state=42)\n","\n","# Convert to PyTorch tensors\n","X = torch.FloatTensor(X_np)\n","y = torch.FloatTensor(y_np)\n","\n","# Visualize the data\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_np[:, 0], X_np[:, 1], c=y_np, cmap='coolwarm', alpha=0.7)\n","plt.title('Binary Classification Data')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.colorbar()\n","plt.show()\n","\n","# Print shapes\n","print(f\"X shape: {X.shape}\")  # [1000, 2]\n","print(f\"y shape: {y.shape}\")  # [1000]"],"metadata":{"id":"zWEhFhLds9LW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### 2. Creating a Custom Dataset and DataLoader\n","\n","Now, let's organize our data using PyTorch's Dataset and DataLoader:"],"metadata":{"id":"JeYqHG8-vA5F"}},{"cell_type":"code","source":["# your code here...."],"metadata":{"id":"e4qYHZvDvILI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BlobDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        return self.features[idx], self.labels[idx]\n","\n","# Create train/test split (80/20)\n","train_size = int(0.8 * len(X))\n","test_size = len(X) - train_size\n","\n","# Random split\n","indices = torch.randperm(len(X))\n","train_indices = indices[:train_size]\n","test_indices = indices[train_size:]\n","\n","# Create datasets\n","train_dataset = BlobDataset(X[train_indices], y[train_indices])\n","test_dataset = BlobDataset(X[test_indices], y[test_indices])\n","\n","# Create dataloaders\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Verify the dataloaders\n","for features, labels in train_loader:\n","    print(f\"Batch features shape: {features.shape}\")\n","    print(f\"Batch labels shape: {labels.shape}\")\n","    break"],"metadata":{"id":"MM8jCMhVvDqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### 3. Building the Logistic Regression Model\n","\n","Now, let's define our logistic regression model using `torch.nn`:"],"metadata":{"id":"Y61091zjvNNk"}},{"cell_type":"code","source":["# your code here...."],"metadata":{"id":"UJ4cjcxpvPMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LogisticRegression(nn.Module):\n","    def __init__(self, input_dim):\n","        super(LogisticRegression, self).__init__()\n","        # Linear layer followed by sigmoid for binary classification\n","        self.linear = nn.Linear(input_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        outputs = self.linear(x)\n","        outputs = self.sigmoid(outputs)\n","        return outputs.squeeze()  # Remove extra dimension\n","\n","# Initialize the model\n","model = LogisticRegression(n_features)\n","print(model)\n","\n","# Examine model parameters\n","for name, param in model.named_parameters():\n","    print(f\"Parameter {name}: {param.shape}\")"],"metadata":{"id":"sdOJU9NdvSv-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Setting up Loss Function and Optimizer\n","\n","For binary classification with logistic regression, we'll use Binary Cross Entropy loss"],"metadata":{"id":"C7hAOGsLvZqF"}},{"cell_type":"code","source":["# your code here...."],"metadata":{"id":"9Gs9AoJBveq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Binary Cross Entropy Loss\n","criterion = nn.BCELoss()  # Binary Cross Entropy\n","\n","# Adam optimizer\n","learning_rate = 0.001\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"jziW0-eMvbHR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Training Loop\n","\n","Now let's train the model:"],"metadata":{"id":"zvJZFzVUviBh"}},{"cell_type":"code","source":["# your code here...."],"metadata":{"id":"n2eYa_bTvijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, criterion, optimizer, num_epochs=100):\n","    # Training statistics\n","    train_losses = []\n","\n","    # Set model to training mode\n","    model.train()\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","\n","        # Iterate over batches\n","        for features, labels in train_loader:\n","            # Forward pass\n","            outputs = model(features)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        # Calculate average loss for the epoch\n","        epoch_loss = running_loss / len(train_loader)\n","        train_losses.append(epoch_loss)\n","\n","        # Print statistics every 10 epochs\n","        if (epoch+1) % 10 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n","\n","    return train_losses\n","\n","# Train the model\n","num_epochs = 100\n","train_losses = train_model(model, train_loader, criterion, optimizer, num_epochs)\n","\n","# Plot the training loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, num_epochs+1), train_losses, marker='o', linestyle='-', markersize=3)\n","plt.title('Training Loss Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"jqoQsGl7vkgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Evaluation and Visualization\n","\n","Let's evaluate our model's performance and visualize the decision boundary:\n"],"metadata":{"id":"59KhTaPevpOv"}},{"cell_type":"code","source":["# your code here...."],"metadata":{"id":"tQkJ_2UIvr5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation function\n","def evaluate_model(model, test_loader, criterion):\n","    model.eval()  # Set model to evaluation mode\n","\n","    test_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # No gradients needed\n","        for features, labels in test_loader:\n","            # Forward pass\n","            outputs = model(features)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item()\n","\n","            # Calculate accuracy\n","            predicted = (outputs > 0.5).float()  # Convert to 0 or 1 based on threshold\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    # Calculate average loss and accuracy\n","    avg_loss = test_loss / len(test_loader)\n","    accuracy = correct / total\n","\n","    return avg_loss, accuracy\n","\n","# Evaluate the model\n","test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n","print(f'Test Loss: {test_loss:.4f}')\n","print(f'Test Accuracy: {test_accuracy:.4f}')\n","\n","# Visualize the decision boundary\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values with some margin\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","\n","    # Create a mesh grid\n","    h = 0.02  # Step size\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","\n","    # Flatten the grid points\n","    grid_tensor = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n","\n","    # Make predictions for the grid points\n","    model.eval()\n","    with torch.no_grad():\n","        Z = model(grid_tensor)\n","        Z = (Z > 0.5).float()\n","\n","    # Reshape the predictions\n","    Z = Z.numpy().reshape(xx.shape)\n","\n","    # Plot the contour and training points\n","    plt.figure(figsize=(10, 8))\n","    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', alpha=0.8)\n","    plt.title('Decision Boundary of Logistic Regression')\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.show()\n","\n","# Plot the decision boundary\n","plot_decision_boundary(model, X_np, y_np)"],"metadata":{"id":"MJtOvXpkvsVK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. Making Predictions on New Data\n","\n","Finally, let's see how to use our trained model for predictions:\n","\n"],"metadata":{"id":"Tn36RytOuf0G"}},{"cell_type":"code","source":["# your code here...."],"metadata":{"id":"RrobVcx6v4uF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_sample(model, features):\n","    # Ensure features are a tensor\n","    if not isinstance(features, torch.Tensor):\n","        features = torch.FloatTensor(features)\n","\n","    # Reshape if it's a single sample\n","    if features.dim() == 1:\n","        features = features.unsqueeze(0)\n","\n","    # Make prediction\n","    model.eval()\n","    with torch.no_grad():\n","        prediction = model(features)\n","        class_prediction = (prediction > 0.5).float()\n","\n","    return prediction.item(), class_prediction.item()\n","\n","# Create a new sample for prediction\n","new_sample = torch.tensor([3.0, 2.0])\n","probability, predicted_class = predict_sample(model, new_sample)\n","\n","print(f\"New sample: {new_sample}\")\n","print(f\"Probability of class 1: {probability:.4f}\")\n","print(f\"Predicted class: {int(predicted_class)}\")"],"metadata":{"id":"9aKn-92Bv4Kt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### n-dimensional Classifier"],"metadata":{"id":"01noGVea-kE6"}},{"cell_type":"markdown","source":["## Dense layer"],"metadata":{"id":"fKAZGizz-T6L"}},{"cell_type":"code","source":["from torchvision.transforms import ToTensor\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}"],"metadata":{"id":"8eUtBi1B-X1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    img, label = training_data[sample_idx]\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()"],"metadata":{"id":"8xWSWqJiM11W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn_to_device(batch, device):\n","    \"\"\"Custom collate function that sends data to specified device\"\"\"\n","    data = torch.stack([item[0] for item in batch]).view(-1, n_features)\n","    targets = torch.tensor([item[1] for item in batch])\n","    return data.to(device), targets.to(device)"],"metadata":{"id":"H9hjrtoRqkfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, collate_fn=lambda batch: collate_fn_to_device(batch, device))\n","test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, collate_fn=lambda batch: collate_fn_to_device(batch, device))"],"metadata":{"id":"bZf7kmvJ-6vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you code: define a neural nework for calssification 10 lables"],"metadata":{"id":"AxEtSliNQWP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiClassClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes, n_hidden=128):\n","        \"\"\"\n","        Multi-class classifier using softmax\n","\n","        Args:\n","            input_dim: Number of input features\n","            num_classes: Number of classes to predict\n","        \"\"\"\n","        super(MultiClassClassifier, self).__init__()\n","\n","        # Linear layer that maps from input_dim -> num_classes\n","        self.linear = nn.Sequential(\n","            nn.Linear(input_dim, n_hidden),\n","            nn.ReLU(),\n","            nn.Linear(n_hidden, num_classes)\n","\n","        )\n","\n","        # Note: We don't include softmax here because CrossEntropyLoss\n","        # applies softmax internally for numerical stability\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the model\n","\n","        Args:\n","            x: Input tensor of shape [batch_size, input_dim]\n","\n","        Returns:\n","            Logits of shape [batch_size, num_classes]\n","        \"\"\"\n","        # Apply linear layer to get logits\n","        logits = self.linear(x)\n","        return logits\n","\n","    def predict_proba(self, x):\n","        \"\"\"\n","        Get probability predictions\n","\n","        Args:\n","            x: Input tensor\n","\n","        Returns:\n","            Probabilities after softmax\n","        \"\"\"\n","        with torch.no_grad():\n","            logits = self.forward(x)\n","            probs = torch.softmax(logits, dim=1)\n","        return probs"],"metadata":{"id":"ZwoOXN1NOU_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: initilize (model, optimizer, criterion)"],"metadata":{"id":"xd6PwRnhQds4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_features = training_data[0][0].numel() # 28*28=784\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model = MultiClassClassifier(n_features, 10).to(device)\n","print(model)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n"],"metadata":{"id":"XvmZksZG-dY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: write trainilin loop"],"metadata":{"id":"wuF-uMbZQkd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","for epoch in range(num_epochs):\n","  losses = []\n","  for x_batch, y_batch in train_dataloader:\n","    optimizer.zero_grad()                       # discard all previous gradients in the computation graph\n","    x = x_batch # flatten and move to device if GPU available\n","    logits = model(x)                           # forward pass (predict)\n","    loss = criterion(logits, y_batch)           # CrossEntropyLoss executes the softmax transformation internally\n","    loss.backward()                             # compute gradients for each parameter in the network\n","    optimizer.step()                            # apply one batch update to the network parameters using the computed gradients\n","    losses.append(loss.item())\n","  if (epoch % (num_epochs//10)) == 0:\n","    print(f\"Epoch {epoch}, average loss: {np.mean(losses):.3f}\")\n","\n"],"metadata":{"id":"188Vc1agJMi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["losses = []                                # Will store loss value for each batch\n","success = []                               # Will store individual prediction results (1 for correct, 0 for incorrect)\n","\n","# Iterate through batches in the test dataloader\n","for x_batch, y_batch in test_dataloader:\n","\n","  logits = model(x_batch.view(-1, n_features).to(device))  # Forward pass: reshape input if needed and move to device (GPU/CPU)\n","  pred_y = torch.argmax(logits, dim=1)                     # Get predicted class by finding index of maximum value along dimension 1\n","  # Calculate loss value for this batch\n","  loss = criterion(logits, y_batch)\n","  losses.append(loss.item())\n","  success.extend((pred_y == y_batch).float().tolist())    # Compare predictions with true labels and convert to float (1.0 for correct, 0.0 for incorrect)\n","print(f\"Test loss: {np.mean(losses):.3f}, Success Rate: {sum(success)/len(success):.3f}\")"],"metadata":{"id":"ZbxVtB7DLvKE"},"execution_count":null,"outputs":[]}]}