{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5338f1f5",
   "metadata": {},
   "source": [
    "# Homework 2: Multiclass Classification with PyTorch\n",
    "\n",
    "In this assignment, you will build, train, and evaluate a neural network for multiclass classification using PyTorch.\n",
    "You will use the [Garbage dataset](https://www.kaggle.com/datasets/mostafaabla/garbage-classification).\n",
    "The goal is to gain hands-on experience with:\n",
    "- Dataset preparation \n",
    "- Building two  PyTorch models\n",
    "- Loss functions for multiclass\n",
    "- Training loop and evaluation\n",
    "- Visualize of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf82d3",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "### Context\n",
    "This dataset has 15,150 images from 12 different classes of household garbage; paper, cardboard, biological, metal, plastic, green-glass, brown-glass, white-glass, clothes, shoes, batteries, and trash.\n",
    "\n",
    "Garbage Recycling is a key aspect of preserving our environment. To make the recycling process possible/easier, the garbage must be sorted to groups that have similar recycling process. I found that most available data sets classify garbage into a few classes (2 to 6 classes at most). Having the ability to sort the household garbage into more classes can result in dramatically increasing the percentage of the recycled garbage.\n",
    "\n",
    "### Content\n",
    "An ideal setting for data collection would be to place a camera above a conveyor where the garbage comes one by one, so that the camera can capture real garbage images. But since such a setup is not feasible at the moment I collected most of the images in this dataset by web scraping, I tried to get images close to garbage images whenever possible, for example in biological garbage category I searched for rotten vegetables, rotten fruits and food remains, etc. However, for some classes such as clothes or shoes it was more difficult to get images of clothes or shoes from the garbage, so mostly it was images of normal clothes. Nevertheless, being able to classify the images of this data set to 12 classes can be a big step towards improving the recycling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f150469",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import kagglehub\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef162a",
   "metadata": {},
   "source": [
    "### Dwonload and prepare dataset from kagglehub\n",
    "`kagglehub.dataset_download` downloads and extracts Kaggle datasets to a local cache directory (usually under `~/.cache/kagglehub/datasets/`). It returns the path to the unzipped dataset, preserving the original folder structure as found on Kaggle, such as one subfolder per class for image datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is the structure of the downloaded content?**\n",
    "\n",
    "* Inside the returned directory (`path`), you will find the files and folders as originally organized on Kaggle.\n",
    "* For the **garbage classification** dataset, you typically get a folder like:\n",
    "\n",
    "  ```\n",
    "  garbage_classification/\n",
    "      cardboard/\n",
    "      glass/\n",
    "      metal/\n",
    "      paper/\n",
    "      plastic/\n",
    "      trash/\n",
    "      ...\n",
    "  ```\n",
    "\n",
    "  Each subfolder contains images belonging to that class (a classic structure for use with `torchvision.datasets.ImageFolder`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1087d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image transform: Compose(\n",
      "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Download the latest version of the Kaggle dataset to a local directory\n",
    "path = kagglehub.dataset_download(\"mostafaabla/garbage-classification\",)\n",
    "\n",
    "# Set the data directory to the location of the downloaded images\n",
    "data_dir = os.path.join(path, \"garbage_classification\")\n",
    "\n",
    "# Set the desired image size for resizing\n",
    "img_size = 64\n",
    "\n",
    "# Define the image transformations to apply to each image:\n",
    "# - Resize the image to (img_size, img_size)\n",
    "# - Convert the image to a PyTorch tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  # Resize images\n",
    "    transforms.ToTensor()                     # Convert to tensor\n",
    "])  \n",
    "\n",
    "print(f\"image transform: {transform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed197d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 15515 number of classes: 12\n",
      "Class names: ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n"
     ]
    }
   ],
   "source": [
    "dataset  = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "classes_names = dataset.classes\n",
    "# dataset = ImageFolder(data_dir + '/Training', transform=transform)\n",
    "print(\"Number of images:\", len(dataset), \"number of classes:\", len(classes_names))\n",
    "print(\"Class names:\", classes_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f0e1e",
   "metadata": {},
   "source": [
    "### TODO 1:\n",
    "Create a 4Ã—3 subplot that displays one example image from each category in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2da23",
   "metadata": {},
   "source": [
    "### TODO 2:\n",
    "Shuffle the dataset and split it into training and validation sets, using 80% of the samples for training and 20% for validation. Make sure that the class distribution is preserved as much as possible in both splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15408b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40025d8",
   "metadata": {},
   "source": [
    "### TODO 3:\n",
    "Visualize the class distribution in both the training and validation sets using a bar plot, so you can compare how well the splits represent the overall dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f391e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645449fc",
   "metadata": {},
   "source": [
    "### TODO 4:\n",
    "Ensure that no single category accounts for more than 15% of the samples in the training set. If necessary, downsample the dominant classes. Then, visualize the new class distribution in the training set using a bar plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201f8e1",
   "metadata": {},
   "source": [
    "## Implementation of Regularization Layers\n",
    "\n",
    "Implement two regularization layers from scratch:\n",
    "1. `BatchNorm2d`\n",
    "2. `LayerNorm`\n",
    "\n",
    "Make sure that all trainable parameters (such as scale and shift) are properly registered as part of the computational graph, so they are optimized during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8360fb",
   "metadata": {},
   "source": [
    "### TODO 5:\n",
    "Implement the BatchNorm2d layer from scratch using only basic PyTorch components (such as `nn.Module` and tensor operations), without relying on `nn.BatchNorm2d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f193d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72548b",
   "metadata": {},
   "source": [
    "### TODO 6:\n",
    "Implement the LayerNorm layer from scratch using only basic PyTorch components (such as `nn.Module` and tensor operations), without relying on `nn.LayerNorm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd3273",
   "metadata": {},
   "source": [
    "## Traning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855054a3",
   "metadata": {},
   "source": [
    "### TODO 7:\n",
    "Complete the `GarbageClassifier` neural network by designing and implementing an architecture of your choice.  \n",
    "Make use of the provided `_block` and `_block_mp` building blocks as you see fit.  \n",
    "Allow the regularization type (e.g., `BatchNorm2d` or `LayerNorm`) to be specified from outside the class, so you can later compare the results between the two types of regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GarbageClassifier, self, norm_layer).__init__()\n",
    "        # self.features = nn.Sequential(\n",
    "        #    your code here\n",
    "        # )\n",
    "\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #    your code here\n",
    "        # )\n",
    "\n",
    "\n",
    "       \n",
    "    def _block_mp(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm_layer=None, kernel_size_mp=2):\n",
    "        \"\"\"AlexNet style block with max pooling\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            norm_layer(out_channels), # Normalization layer that you have implemented\n",
    "            nn.MaxPool2d(kernel_size=kernel_size_mp),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm_layer=None):\n",
    "        \"\"\"AlexNet style block without max pooling\"\"\"\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature extractor givven self.features and self.classifier\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204f191",
   "metadata": {},
   "source": [
    "### TODO 8:\n",
    "Prepare all components needed for training:\n",
    "1. Build your neural network with one type of regularization.\n",
    "2. Create DataLoaders for the training (and optionally validation) sets.\n",
    "3. Define the loss criterion.\n",
    "4. Define the optimizer and assign it the trainable parameters of your model.\n",
    "5. Print a summary of your network architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your conde here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65289e",
   "metadata": {},
   "source": [
    "### TODO 9:\n",
    "Write a training loop to train your network for 10 epochs using the training set.\n",
    "- Track and print the training loss for each epoch.\n",
    "- After each epoch, compute and store both the loss and accuracy on the test set.\n",
    "- After training, plot both the training and test losses on the same graph to visualize the learning process.\n",
    "- Your model should achieve at least 75% accuracy on the test set.\n",
    "- Remember to set your model to training mode (`model.train()`) during training, and to evaluation mode (`model.eval()`) when computing metrics on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353319f",
   "metadata": {},
   "source": [
    "### TODO 10:\n",
    "Compute and report the accuracy of your trained model on the test set for each individual category (class).\n",
    "\n",
    "For example:\n",
    "* Class battery: 80%\n",
    "* Class biological: 71%\n",
    "* Class brown-glass: 70%\n",
    "* Class cardboard: 85%\n",
    "* Class clothes: 92%\n",
    "* Class green-glass: 88%\n",
    "* Class metal: 43%\n",
    "* Class paper: 54%\n",
    "* Class plastic: 39%\n",
    "* Class shoes: 71%\n",
    "* Class trash: 68%\n",
    "* Class white-glass: 55%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395350b4",
   "metadata": {},
   "source": [
    "### Repeat TODOs 8,9 and 10 with the other type of regularization\n",
    "\n",
    "Repeat the previous steps for preparing your model, DataLoaders, optimizer, and training loop, but this time using the alternative regularization layer. After training, compare the performance between the two regularization types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fde44",
   "metadata": {},
   "source": [
    "### Final Task: Summary and Analysis\n",
    "\n",
    "Write a summary of your work and the results you obtained. In 3â€“4 paragraphs, discuss your approach, key findings, and any challenges you encountered. Compare the performance of the two different regularization techniques you implemented, and suggest possible reasons for any differences you observed. Reflect on what you learned and what you might try differently in future experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de9f0c",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
