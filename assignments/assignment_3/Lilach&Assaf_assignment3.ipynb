{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9cc8f0",
   "metadata": {},
   "source": [
    "# Students: Lilach pardess & Assaf Taubenfeld\n",
    "\n",
    "NOTE: we removed all the instractions to have the file more readable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6a1d0dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7789cbfad790>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter, deque\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3794e0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 5436475 characters\n",
      "Number of unique characters: 84\n",
      "First 100 characters:\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as \n"
     ]
    }
   ],
   "source": [
    "# Loading the shakespeare.txt file\n",
    "\n",
    "f_name = 'shakespeare.txt' # if the file in different location, change this line\n",
    "with open('shakespeare.txt', \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Length of text: {len(text)} characters\")\n",
    "print(f\"Number of unique characters: {len(set(text))}\")\n",
    "print(f\"First 100 characters:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d7918b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 84\n",
      "Original: To be or not to be\n",
      "Encoded: [45, 70, 1, 57, 60, 1, 70, 73, 1, 69, 70, 75, 1, 75, 70, 1, 57, 60]\n",
      "Decoded: To be or not to be\n"
     ]
    }
   ],
   "source": [
    "# Define CharacterTokenizer \n",
    "\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer by building vocabulary from input text.\n",
    "        \n",
    "        Args:\n",
    "            text: String containing all training text\n",
    "        \"\"\"\n",
    "        # Get all unique characters and sort them\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        mapping_list = list(enumerate(self.chars))\n",
    "        self.char_to_index = {char: index for index, char in mapping_list}\n",
    "        self.index_to_char = {index: char for index, char in mapping_list}\n",
    " \n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert string to list of integers\"\"\"\n",
    "        return [self.char_to_index[char] for char in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert list of integers back to string\"\"\"\n",
    "        return ''.join([self.index_to_char[index] for index in indices])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary\"\"\"\n",
    "        return self.vocab_size\n",
    "\n",
    "# Assuming your text is loaded in a variable called 'text'\n",
    "# Create the tokenizer\n",
    "tokenizer = CharacterTokenizer(text)\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_text = \"To be or not to be\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Tokenizer vocab_size:\", len(tokenizer))\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191430e",
   "metadata": {},
   "source": [
    "### Explanation [CharacterTokenizer]\n",
    "\n",
    "Implementing characther based tokenizer.\n",
    "\n",
    "init: create 2 maps, from char to index and from index to char. this will allow to encode and decode the characthers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a55cc32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of character frequencies\n",
    "char_counts = Counter(text)\n",
    "def plot_char_distribution(char_counts):\n",
    "    \"\"\"Plot the distribution of character frequencies.\"\"\"\n",
    "    chars, counts = zip(*char_counts.most_common(30))  # Get top 30 characters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(chars, counts)\n",
    "    plt.xlabel('Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Character Frequency Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "plot_char_distribution(char_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2827f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([5436475])\n",
      "Training set size: 4,892,827 characters\n",
      "Validation set size: 543,648 characters\n"
     ]
    }
   ],
   "source": [
    "#Prepare dataset for training\n",
    "\n",
    "# Convet the encoded text to torch tensor\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "\n",
    "# Split into train and validation sets (90/10 split)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Training set size: {len(train_data):,} characters\")\n",
    "print(f\"Validation set size: {len(val_data):,} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18c85788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Shakespeare text data.\n",
    "    Generates overlapping sequences for language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, block_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Tensor of encoded text tokens\n",
    "            block_size: Length of each sequence (context length)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Calculate number of possible sequences\n",
    "        self.num_sequences = len(data) - block_size\n",
    "        \n",
    "        if self.num_sequences <= 0:\n",
    "            raise ValueError(f\"Data length ({len(data)}) must be greater than block_size ({block_size})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of possible sequences\"\"\"\n",
    "        return self.num_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sequence and its target.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sequence start position\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (input_sequence, target_sequence)\n",
    "        \"\"\"\n",
    "        # Extract input sequence of length block_size starting at idx\n",
    "        input_sequence = self.data[idx:idx + self.block_size]\n",
    "        \n",
    "        # Extract target sequence (shifted by 1 position)\n",
    "        target_sequence = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        \n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b446487c",
   "metadata": {},
   "source": [
    "###  Explanation [ShakespeareDataset]: \n",
    "\n",
    "The getitem cant go out of bounds because how the dataloader of pytourch is implemented. the __len_ ensure that only valid idx will be provided to the getitem method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f195c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences available: 4,892,699\n",
      "Validation sequences available: 543,520\n",
      "==================================================\n",
      "Example of a training batch instance:\n",
      "\n",
      "Input:'e, where doing tends to ill,\n",
      "    The truth is then most done not doing it;\n",
      "    The better act of purposes mistook\n",
      "    Is to mist'\n",
      "==================================================\n",
      "Target:', where doing tends to ill,\n",
      "    The truth is then most done not doing it;\n",
      "    The better act of purposes mistook\n",
      "    Is to mista'\n"
     ]
    }
   ],
   "source": [
    "# Example datset class\n",
    "\n",
    "num_workers = 2\n",
    "shuffle_train = True\n",
    "block_size = 128\n",
    "batch_size = 16\n",
    "train_dataset = ShakespeareDataset(train_data, block_size)\n",
    "val_dataset = ShakespeareDataset(val_data, block_size)\n",
    "\n",
    "print(f\"Training sequences available: {len(train_dataset):,}\")\n",
    "print(f\"Validation sequences available: {len(val_dataset):,}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_train,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True  # Drop last incomplete batch for consistent batch sizes\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Example of a training batch instance:\", end='\\n\\n')\n",
    "print(f\"Input:'{tokenizer.decode(x[0].tolist())}'\") # Decode the first input sequence\n",
    "print(\"=\" * 50)\n",
    "print(f\"Target:'{tokenizer.decode(y[0].tolist())}'\") # Decode the first target sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efa430",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Feed Forward Netroks. simple feed-forward component that process the output of the attention layer.\n",
    "The MLp layer streach the dimention of the embedding from `embed_size` to `4*embed_size` and then back to `embed_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7f2be3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP with two linear layers and GELU activation.\n",
    "    First layer: embed_size -> 4 * embed_size\n",
    "    Second layer: 4 * embed_size -> embed_size\n",
    "    GELU activation between the two layers\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.embed_size = config.embed_size\n",
    "        self.layer_one = nn.Linear(self.embed_size, 4 * self.embed_size)\n",
    "        self.layer_two = nn.Linear(4 * self.embed_size, self.embed_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_one(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.layer_two(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9d3a1",
   "metadata": {},
   "source": [
    "### Multi Head Masked Self-Attention Implementation\n",
    "\n",
    "The self-attention mechanism which is the core of the transformer architecture, allowing the model to weigh the importance of different words in a sequence relative to each other. This guide walks you through completing the provided implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f163eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        assert config.embed_size % config.num_heads == 0\n",
    "        \n",
    "        # self-attention values (key, query, value). pay attention that head_size * num_heads must be equal to embed_size\n",
    "        self.c_atten = nn.Linear(config.embed_size, 3*config.embed_size) # multiply by 3 to get k, q, v\n",
    "        \n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.embed_size, config.embed_size)\n",
    "        \n",
    "        self.embed_size = config.embed_size\n",
    "        self.num_heads = config.num_heads\n",
    "\n",
    "        mask = torch.tril(torch.ones(config.block_size, config.block_size)) # Lower triangular mask for causal attention\n",
    "        mask = mask.view(1, 1, config.block_size, config.block_size) # B, 1, T, T\n",
    "        self.register_buffer('mask', mask)  # register_buffer allows the mask to be part of the model state but not a parameter to optimize\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #  after each line print the shape of x\n",
    "        \"\"\"        Forward pass for the self-attention block.\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, C) where B is batch size, T is sequence length, C is embedding size\n",
    "        Returns:\n",
    "            out: Output tensor of shape (B, T, C) after self-attention and projection\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape # Batch, sequence length, embed_size\n",
    "        if C != self.embed_size:\n",
    "            raise ValueError(f\"Input embedding size {C} does not match expected embedding size {self.embed_size}\")\n",
    "        \n",
    "        k, q, v = self.c_atten(x).split(self.embed_size, dim=2) # split the output into k, q, v.  Each is of shape (B, T, embed_size)\n",
    "        head_size = self.embed_size // self.num_heads\n",
    "\n",
    "        # reshape k, q, v to (B, T, num_heads, head_size). As embed_size == num_heads * head_size, we can do this by dividing embed_size by num_heads\n",
    "        k = k.view(B, T, self.num_heads, head_size)\n",
    "        q = q.view(B, T, self.num_heads, head_size)\n",
    "        v = v.view(B, T, self.num_heads, head_size)\n",
    "\n",
    "        # transpose k, q, v to (B, num_heads, T, head_size) \n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # compute attention scores\n",
    "        att_scores = (q @ k.transpose(-2, -1))\n",
    "        att_scores = att_scores / math.sqrt(head_size) # scale the attention scores\n",
    "        att_scores = att_scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf')) # mask the attention scores\n",
    "        \n",
    "        # calculte the attention weights\n",
    "        att_weights = F.softmax(att_scores, dim=-1) # shape (B, num_heads, T, T)\n",
    "\n",
    "        # compute the output\n",
    "        output = att_weights @ v # shape (B, num_heads, T, head_size)\n",
    "\n",
    "        # concatenate the output of the heads\n",
    "        concatenated_output = output.transpose(1, 2).contiguous().view(B, T, C) # shape (B, T, C)\n",
    "\n",
    "        # project the output to the embed_size\n",
    "        final_output = self.c_proj(concatenated_output)\n",
    "\n",
    "        return final_output # shape (B, T, embed_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da9c70",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "The transformer block is a single layer containing two main components: **Multi-Head Attention** followed by **MLP (Multi-Layer Perceptron)**. This layer design gets stacked multiple times to create the full transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d00b5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_size)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_size)\n",
    "        self.mlp = Mlp(config)\n",
    "        self.self_attn = SelfAttentionBlock(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        normalized_attn_input = self.ln1(x)\n",
    "        attn_output = self.self_attn(normalized_attn_input)\n",
    "        x = x + attn_output\n",
    "        normalized_mlp_input = self.ln2(x)\n",
    "        mlp_output = self.mlp(normalized_mlp_input)\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1274aa",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "\n",
    "The `GPT2` class represents the complete transformer model - a decoder-only architecture that processes text and predicts the next token. This is the full model that combines all the components you've built.\n",
    "\n",
    "### Model Architecture Overview\n",
    "\n",
    "**Embedding Layers**: Convert discrete tokens into continuous vector representations that the neural network can process.\n",
    "\n",
    "**Transformer Stack**: Multiple transformer blocks stacked together, each refining the representations further.\n",
    "\n",
    "**Output Layer**: Converts the final representations back to vocabulary predictions for next-token generation.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "**Token Embedding (`wte`)**: Maps each vocabulary word to a learned vector representation. Transforms token IDs into dense embeddings of size `embed_size`.\n",
    "\n",
    "**Position Embedding (`wpe`)**: Adds positional information since attention doesn't inherently understand word order. Each position gets its own learned embedding.\n",
    "\n",
    "**Transformer Blocks**: Stack of identical blocks (typically 12-48 layers) that progressively refine the representations through attention and feed-forward processing.\n",
    "\n",
    "**Final Layer Norm (`ln_f`)**: Normalizes the output from all transformer blocks before making predictions.\n",
    "\n",
    "**Language Model Head (`lm_head`)**: Linear layer that converts final embeddings back to vocabulary logits for next-token prediction.\n",
    "\n",
    "### Forward Pass Flow\n",
    "\n",
    "**Input Processing**:\n",
    "1. Convert token IDs to embeddings\n",
    "2. Add positional embeddings to provide sequence order information\n",
    "3. Combine token and position embeddings element-wise\n",
    "\n",
    "**Transformer Processing**:\n",
    "- Pass through each transformer block sequentially\n",
    "- Each block applies attention and MLP with residual connections\n",
    "- Representations become increasingly sophisticated through the stack\n",
    "\n",
    "**Output Generation**:\n",
    "1. Apply final normalization to stabilize outputs\n",
    "2. Project to vocabulary size using the language model head\n",
    "3. Compute loss against targets if provided for training\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "**Weight Sharing**: The token embedding and output projection share the same weights, reducing parameters and creating symmetry between input and output representations.\n",
    "\n",
    "**Autoregressive Training**: During training, the model predicts each position based only on previous positions, learning the sequential nature of language.\n",
    "\n",
    "**Next-Token Prediction**: The fundamental task is predicting the probability distribution over the vocabulary for what token should come next at each position.\n",
    "\n",
    "This architecture enables the model to generate coherent text by learning statistical patterns in language through the simple but powerful objective of next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e3ba1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.embed_size), # token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.embed_size), # positional embeddings\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)]), # transformer blocks\n",
    "            ln_f = nn.LayerNorm(config.embed_size)\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size) # output layer for next token prediction\n",
    "\n",
    "        # sharing weights between token embedding and output layer\n",
    "        self.transformer.wte.weight = self.lm_head.weight # this allows the model to use the same embeddings for input and output tokens\n",
    "        \n",
    "    def forward(self, idx, target=None):\n",
    "        \"\"\" Forward pass of the GPT-2 model.\n",
    "        Args:            idx: Input tensor of shape (B, T) where B is batch size and T is sequence length\n",
    "            target: Optional target tensor of shape (B, T) for computing loss\n",
    "        Returns:            logits: Output tensor of shape (B, T, vocab_size) containing logits for next token\n",
    "            loss: Optional scalar loss value if target is provided\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Input tokens length {T} exceeds maximum length {self.config.block_size}\"\n",
    "        \n",
    "        token_embeddings = self.transformer.wte(idx)\n",
    "        position_embeddings = self.transformer.wpe(torch.arange(T, device=idx.device))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed0f77",
   "metadata": {},
   "source": [
    "### Text Generation using Top-K sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0954f313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngenerate_text = pipeline(model, tokenizer)\\nprompt = \"To be or not to be, that is the question: \"\\ngenerated_text = generate_text(prompt, max_len=100)\\n'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_k_sample(logits, k=50):\n",
    "    values, indices = torch.topk(logits, k)\n",
    "    probs = torch.softmax(values, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return indices[next_token]\n",
    "\n",
    "def pipeline(model, tokenizer, k=10):\n",
    "    \"\"\"    Create a text generation pipeline for the given model and tokenizer.\n",
    "    Args:\n",
    "        model: The trained model instance\n",
    "        tokenizer: The tokenizer instance\n",
    "        k: Number of top-k tokens to sample from at each step (default: 10)\n",
    "    Returns:A function that takes a prompt and generates text\"\"\"\n",
    "    # step-by-step explanation of the pipeline function:\n",
    "    \"\"\"\n",
    "    1. Set the model to evaluation mode\n",
    "    2. Encode the input prompt using the tokenizer\n",
    "    3. Convert the encoded prompt to a tensor and move it to the model's device\n",
    "    4. While the length of the generated sequence is less than max_len:\n",
    "        a. Get the model's logits for the last token in the sequence\n",
    "        b. Extract the logits for the last token and apply softmax to get probabilities\n",
    "        v. Use top-k sampling to select the next token based on probabilities\n",
    "        d. Append the selected token to the sequence\n",
    "    5. Decode the generated sequence back to text using the tokenizer\n",
    "    6. Return the generated text\n",
    "    \"\"\"\n",
    "    def generate(prompt, max_len=1024):\n",
    "\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device \n",
    "        encoded_prompt = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device)\n",
    "        encoded_prompt = encoded_prompt.unsqueeze(0)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            with torch.no_grad():\n",
    "                logits, _ = model(encoded_prompt)\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                next_token = top_k_sample(next_token_logits, k=10)\n",
    "\n",
    "                # Update the squence with the new token\n",
    "                encoded_prompt = torch.cat([encoded_prompt, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        \n",
    "        return tokenizer.decode(encoded_prompt[0].tolist())\n",
    "\n",
    "\n",
    "# exmple usage of the pipeline\n",
    "\"\"\"\n",
    "generate_text = pipeline(model, tokenizer)\n",
    "prompt = \"To be or not to be, that is the question: \"\n",
    "generated_text = generate_text(prompt, max_len=100)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dff197",
   "metadata": {},
   "source": [
    "### Conig the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ee3dd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most minmal config class for GPT-2 (Takes 40-60 to train). You are welcome to modify it to a bigger model or for loger but no the other way around (expet of batch size which you can reduce if you run out of memory)\n",
    "class GPT2Config:\n",
    "    def __init__(self):\n",
    "        self.embed_size = 64\n",
    "        self.num_heads = 2\n",
    "        self.num_layers = 4\n",
    "        self.vocab_size = len(tokenizer)\n",
    "        self.block_size = 256\n",
    "        self.lr = 3e-4\n",
    "        self.batch_size = 32 # use a smaller batch size if you run out of memory\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.path = 'models/model.pth'\n",
    "        self.num_epochs = 10\n",
    "\n",
    "        self.patience = 5  # Early stopping patience\n",
    "        self.grad_clip = 1.0  # Gradient clipping threshold\n",
    "        \n",
    "        # Learning rate scheduler configs\n",
    "        self.use_scheduler = True\n",
    "        self.scheduler_type = 'reduce_on_plateau'  # 'reduce_on_plateau', 'cosine', 'step'\n",
    "        self.lr_patience = 2  # For ReduceLROnPlateau\n",
    "        self.lr_factor = 0.5  # For ReduceLROnPlateau\n",
    "        self.step_size = 1  # For StepLR\n",
    "        self.gamma = 0.1  # For StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202876e",
   "metadata": {},
   "source": [
    "## Traninig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f822ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traning function\n",
    "def train_model(model, train_loader, val_loader, optimizer, config=None):\n",
    "\n",
    "    # Setup\n",
    "    model.train()\n",
    "    summary_writer = SummaryWriter(log_dir='runs/shakespeare_experiment')\n",
    "    \n",
    "    # Create models directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Training state tracking\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = getattr(config, 'patience', 5)  # Early stopping patience\n",
    "    \n",
    "    # Loss tracking\n",
    "    train_losses = deque(maxlen=1000)\n",
    "    global_step = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Training for {config.num_epochs} epochs\")\n",
    "    print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\") as pbar:\n",
    "            for batch_idx, (x, y) in enumerate(pbar):\n",
    "                x, y = x.to(config.device), y.to(config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                logits, loss = model(x, y)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping (optional but recommended)\n",
    "                if hasattr(config, 'grad_clip') and config.grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                loss_item = loss.item()\n",
    "                train_losses.append(loss_item)\n",
    "                epoch_train_losses.append(loss_item)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f\"{loss_item:.4f}\",\n",
    "                    'avg_loss': f\"{np.mean(train_losses):.4f}\",\n",
    "                    'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "                })\n",
    "                \n",
    "                # Log to tensorboard every N steps\n",
    "                if global_step % 10 == 0:  # Log every 10 steps\n",
    "                    summary_writer.add_scalar('Loss/train_step', loss_item, global_step)\n",
    "                    summary_writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                \n",
    "                global_step += 1\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\", leave=False) as val_pbar:\n",
    "                for x, y in val_loader:\n",
    "                    x, y = x.to(config.device), y.to(config.device)\n",
    "                    logits, loss = model(x, y)\n",
    "                    val_losses.append(loss.item())\n",
    "                    val_pbar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        summary_writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)\n",
    "        summary_writer.add_scalar('Loss/validation_epoch', avg_val_loss, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "        # Model checkpointing\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'global_step': global_step\n",
    "        }\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(checkpoint, os.path.join('models', 'best_model.pt'))\n",
    "            print(f\"New best model saved! Val loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        torch.save(checkpoint, os.path.join('models', f'checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        # Save latest model using your custom save method\n",
    "        if hasattr(model, 'save_model'):\n",
    "            model.save_model(config.path)\n",
    "    \n",
    "    # Final logging\n",
    "    summary_writer.close()\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_train_loss': avg_train_loss,\n",
    "        'final_val_loss': avg_val_loss,\n",
    "        'epochs_trained': epoch + 1\n",
    "    }\n",
    "\n",
    "def estimate_loss(model, eval_iters, data_loaders={}):\n",
    "    \"\"\"        Estimate the loss of the model on the given splits.\n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        eval_iters: Number of iterations to average the loss over\n",
    "        splits: List of dataset splits to evaluate (e.g., 'train', 'validation')\n",
    "        data_loader: DataLoader instance for fetching batches\n",
    "    Returns:\n",
    "        out: Dictionary with average loss for each split\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split, dl in data_loaders.items():\n",
    "            losses = 0\n",
    "            for i in range(eval_iters):\n",
    "                x, y = next(iter(dl))\n",
    "                x, y = x.to(model.config.device), y.to(model.config.device)\n",
    "                _, loss = model(x, y)\n",
    "                losses += loss.item()\n",
    "            out[split] = losses / eval_iters\n",
    "        model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a5bfaee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Total parameters: 221,908\n"
     ]
    }
   ],
   "source": [
    "# init config and datasets\n",
    "con = GPT2Config()\n",
    "train_dataset = ShakespeareDataset(train_data, con.block_size)\n",
    "val_dataset = ShakespeareDataset(val_data, con.block_size)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=con.batch_size,\n",
    "    shuffle=shuffle_train,\n",
    "    num_workers=4,\n",
    "    drop_last=True  # Drop last incomplete batch for consistent batch sizes\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=con.batch_size,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=4,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "\n",
    "model = GPT2(con)\n",
    "model.to(con.device)\n",
    "print(f\"device: {con.device}\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=con.lr)\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e7dd3da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training for 10 epochs\n",
      "Training batches: 152892, Validation batches: 16981\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 10749/152892 [02:51<38:21, 61.77it/s, loss=1.8524, avg_loss=1.8731, lr=3.00e-04]"
     ]
    }
   ],
   "source": [
    "# Creating an optimizer \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=con.lr, weight_decay=0.01)\n",
    "\n",
    "# train the model\n",
    "results = train_model(model, train_loader, val_loader, optimizer, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "474537c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Training data device: cpu\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Training data device: {next(iter(train_loader))[0].device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "716ea876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"config.device: {con.device}\")  # Should print 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89517e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(model, tokenizer, k=10)\n",
    "print(generator('to be or not to be', max_len=1024)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning IDC",
   "language": "python",
   "name": "deep-learning-idc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
