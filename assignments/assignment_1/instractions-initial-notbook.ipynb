{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SE6qcw_j8Pi2"},"source":["In this homework assignment, you are requested to implement a full backprop algorithm using only *numpy*.\n","\n","- We assume sigmoid activation across all layers.\n","- We assume a single value in the output layer"]},{"cell_type":"code","metadata":{"id":"UV4RvXYL8k85","executionInfo":{"status":"ok","timestamp":1748271720245,"user_tz":-180,"elapsed":9,"user":{"displayName":"Assaf Taubenfeld","userId":"09996670443869298875"}}},"source":["import numpy as np\n","np.random.seed(42)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRml6glFIPCa"},"source":["The following class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network. You should implement the methods (indicated with TODO), that performs forward and backward for an entire batch. Note, the idea is to use matrix multiplications, and not running standard loops over the instances in the batch."]},{"cell_type":"code","metadata":{"id":"kLdNoCt58qg5","executionInfo":{"status":"ok","timestamp":1748271722643,"user_tz":-180,"elapsed":3,"user":{"displayName":"Assaf Taubenfeld","userId":"09996670443869298875"}}},"source":["class MyNN:\n","  def __init__(self, learning_rate, layer_sizes):\n","    '''\n","    learning_rate - the learning to use in backward\n","    layer_sizes - a list of numbers, each number repreents the nuber of neurons\n","                  to have in every layer. Therfore, the length of the list\n","                  represents the number layers this network has.\n","    '''\n","    self.learning_rate = learning_rate\n","    self.layer_sizes = layer_sizes\n","    self.model_params = {}\n","    self.memory = {}\n","    self.grads = {}\n","\n","    # Initializing weights\n","    for layer_index in range(len(layer_sizes) - 1):\n","      W_input = layer_sizes[layer_index + 1]\n","      W_output = layer_sizes[layer_index]\n","      self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1\n","      self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1\n","\n","\n","  def forward_single_instance(self, x):\n","    a_i_1 = x\n","    self.memory['a_0'] = x\n","    for layer_index in range(len(self.layer_sizes) - 1):\n","      W_i = self.model_params['W_' + str(layer_index + 1)]\n","      b_i = self.model_params['b_' + str(layer_index + 1)]\n","      z_i = np.dot(W_i, a_i_1) + b_i\n","      a_i = 1/(1+np.exp(-z_i))\n","      self.memory['a_' + str(layer_index + 1)] = a_i\n","      a_i_1 = a_i\n","    return a_i_1\n","\n","\n","  def log_loss(self, y_hat, y):\n","    '''\n","    Logistic loss, assuming a single value in y_hat and y.\n","    '''\n","    m = y_hat[0]\n","    cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])\n","    return cost\n","\n","\n","  def backward_single_instance(self, y):\n","    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n","    dz = a_output - y\n","\n","    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n","      print(layer_index)\n","      a_l_1 = self.memory['a_' + str(layer_index - 1)]\n","      dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))\n","      self.grads['dW_' + str(layer_index)] = dW\n","      W_l = self.model_params['W_' + str(layer_index)]\n","      dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))\n","      # TODO: calculate and memorize db as well.\n","\n","  # TODO: update weights with grads\n","  #def update(self):\n","\n","  # TODO: implement forward for a batch X.shape = (network_input_size, number_of_instance)\n","  #def forward_batch(self, X)\n","\n","  # TODO: implement backward for a batch y.shape = (1, number_of_instance)\n","  #def backward_batch(self, y)\n","\n","  # TODO: implement log_loss_batch, for a batch of instances\n","  # def log_loss(self, y_hat, y):"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qib6W4QXO644","executionInfo":{"status":"ok","timestamp":1748271725113,"user_tz":-180,"elapsed":3,"user":{"displayName":"Assaf Taubenfeld","userId":"09996670443869298875"}}},"source":["nn = MyNN(0.01, [3, 2, 1])"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nQR8QllPf_5","executionInfo":{"status":"ok","timestamp":1748271725843,"user_tz":-180,"elapsed":12,"user":{"displayName":"Assaf Taubenfeld","userId":"09996670443869298875"}},"outputId":"70cbfe4c-2926-4260-d546-ee8dfc9fc686","colab":{"base_uri":"https://localhost:8080/"}},"source":["nn.model_params"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'W_1': array([[ 0.04967142, -0.01382643,  0.06476885],\n","        [ 0.15230299, -0.02341534, -0.0234137 ]]),\n"," 'b_1': array([0.15792128, 0.07674347]),\n"," 'W_2': array([[-0.04694744,  0.054256  ]]),\n"," 'b_2': array([-0.04634177])}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"VXiyn-yrPC6-","executionInfo":{"status":"ok","timestamp":1748271727514,"user_tz":-180,"elapsed":5,"user":{"displayName":"Assaf Taubenfeld","userId":"09996670443869298875"}},"outputId":"f87a8582-96a3-41cd-b77d-1b3cf6e67268","colab":{"base_uri":"https://localhost:8080/"}},"source":["x = np.random.randn(3)\n","y = np.random.randn(1)\n","\n","y_hat = nn.forward_single_instance(x)\n","print(y_hat)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.48946]\n"]}]},{"cell_type":"code","metadata":{"id":"k5M50i3plclj","executionInfo":{"status":"ok","timestamp":1748271729191,"user_tz":-180,"elapsed":25,"user":{"displayName":"Assaf Taubenfeld","userId":"09996670443869298875"}},"outputId":"c277cd2f-b6bb-459f-ebc6-2c84cc0d563d","colab":{"base_uri":"https://localhost:8080/"}},"source":["nn.backward_single_instance(y)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","1\n"]}]},{"cell_type":"code","metadata":{"id":"CWnZB1YmYnIt"},"source":["def train(X, y, epochs, batch_size):\n","  '''\n","  Train procedure, please note the TODOs inside\n","  '''\n","  for e in range(1, epochs + 1):\n","    epoch_loss = 0\n","    # TODO: shuffle\n","    batches = #... TODO: divide to batches\n","    for X_b, y_b in batches:\n","      y_hat = nn.forward_batch(X_b)\n","      epoch_loss += nn.log_loss_batch(y_hat, y_b)\n","      nn.backward_batch(y_b)\n","      nn.update()\n","    print(f'Epoch {e}, loss={epoch_loss/len(batches)}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cE1ydWlatkty"},"source":["# TODO: Make sure the following network trains properly\n","\n","nn = MyNN(0.001, [6, 4, 3, 1])\n","\n","X = np.random.randn(6, 100)\n","y = np.random.randn(1, 100)\n","batch_size = 8\n","epochs = 2\n","\n","train(X, y, epochs, batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dY4scUksulC"},"source":["#TODO: train on an external dataset\n","\n","Train on the *hour.csv* file with a split of 75% training 10% validation and 15% for test.\n","Use the following features from the data:\n","\n","* temp\n","* atemp\n","* hum\n","* windspeed\n","* weekday\n","\n","The response variable is, *success*\n","\n","The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n","\n","Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n","\n","Then, plot train and validation loss per epoch."]},{"cell_type":"markdown","source":["##  your code goes here"],"metadata":{"id":"ZKAxdO2I1IGT"}},{"cell_type":"markdown","source":["###  Data Preprocessing"],"metadata":{"id":"CHGoWEJk1K7r"}},{"cell_type":"code","source":["# TODO: Preprocess the bike sharing dataset ('hour.csv')\n","# - Load the dataset from the provided hour.csv file\n","# - Select the required features (temp, atemp, hum, windspeed, weekday)\n","# - Extract the target variable (success)\n","# - Normalize/standardize features if necessary\n","# - Split the data into training (75%), validation (10%), and test (15%) sets\n","# - Create DataLoader objects with batch_size=8"],"metadata":{"id":"e4Ra8vc_1NCE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Training\n"],"metadata":{"id":"glp6Vz3B1TZo"}},{"cell_type":"code","source":["# TODO: Train the neural network\n","# - Implement the network with architecture [5, 40, 30, 10, 7, 5, 3, 1]\n","# - Train for exactly 100 epochs on the training set\n","# - Use batch_size=8 as specified\n","# - Calculate and store train and validation loss for each epoch\n","# - Track training progres"],"metadata":{"id":"LXlKl21D1UGn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualization"],"metadata":{"id":"cwBcg1yy1b-j"}},{"cell_type":"code","source":["# TODO: Create visualizations of the learning process\n","# - Plot the training loss per epoch\n","# - Create additional relevant plots (validation loss, learning curves, etc.)\n","# - Make sure all plots have proper labels, titles, and legends\n","# - Add brief analysis of what the plots reveal about your model's performance"],"metadata":{"id":"lqE6Jhxj1etd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Evaluation\n"],"metadata":{"id":"xQIAgWhe1i_2"}},{"cell_type":"code","source":["# TODO: Evaluate model performance on the test set\n","# - Calculate and report the loss on the test set\n","# - Calculate and report the accuracy on the test set\n","# - Compare test performance with training/validation performance\n","# - Analyze model strengths and weaknesses\n","# - Discuss any overfitting/underfitting issues observed"],"metadata":{"id":"IfquqstM1iIO"},"execution_count":null,"outputs":[]}]}